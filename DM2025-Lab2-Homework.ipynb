{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:ÈÇ±ËÅñÂ†Ø\n",
    "\n",
    "Student ID:114061582\n",
    "\n",
    "GitHub ID:224604594\n",
    "\n",
    "Kaggle name: SHENG-YAO QIU\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/competitions/dm-lab-2-private-competition)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "The first goal of this project was to consolidate multiple scattered data sources into a single, clean DataFrame suitable for analysis and model training.\n",
    "\n",
    "The preprocessing pipeline included the following steps:\n",
    "\n",
    "1. Merging multiple raw files into one unified DataFrame\n",
    "All text, labels, tags, and IDs from different files were merged into a single dataset to ensure consistent formatting and easier downstream processing.\n",
    "\n",
    "\n",
    "    <img src=\"./pics/tt_df.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "2. Discarding the tag column\n",
    "Upon inspection, most tag information already appeared directly inside the text.\n",
    "To avoid duplicated semantic signals and reduce noise, the tag field was removed.\n",
    "\n",
    "3. Aligning samples using id-based merging\n",
    "All data sources were merged using id as the primary key to guarantee correct pairing of text and labels across different files.\n",
    "\n",
    "4. Generating a final analyzable dataframe\n",
    "After merging, the dataset contained consistent fields such as id, text, label, split, and later additional engineered features.\n",
    "\n",
    "5. Numerical encoding of target labels\n",
    "Since the original emotion categories were strings, a numerical label mapping was created (e.g., joy=5, anger=0).This enabled compatibility with machine learning classifiers and loss functions.\n",
    "```python\n",
    "    label_id = { \"anger\": 0, \"disgust\": 1, \"fear\": 2,\n",
    "                \"sadness\": 3, \"surprise\": 4, \"joy\": 5 }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "#### 1.2.1 Embedding Features  \n",
    "Multiple embedding models were used to extract semantic features, including:  \n",
    "1. `LLaMA-based embeddings`  \n",
    "2. `MPNet embeddings (final system‚Äôs strongest representation)`  \n",
    "3. `Other transformer embedding variants`  \n",
    "\n",
    "Each sentence is converted into a dense vector (typically 768 dimensions), capturing contextual semantic meaning for model training.\n",
    "\n",
    "#### 1.2.2 TF‚ÄìIDF Features  \n",
    "Word-level TF‚ÄìIDF (1‚Äì2 grams) was used to capture lexical information and important keywords.\n",
    "Character-level TF‚ÄìIDF (3‚Äì5 grams) was added to better capture:  \n",
    "1. `slang`  \n",
    "2. `emojis`  \n",
    "3. `repeated characters`  \n",
    "4. `stylistic emotional patterns`  \n",
    "\n",
    "This resulted in a high-dimensional sparse matrix (20k‚Äì30k features), effectively complementing transformer embeddings.\n",
    "\n",
    "#### 1.2.3 Frequency-based Word Augmentation\n",
    "High-frequency and low-frequency words were analyzed for potential augmentation or removal.\n",
    "Experiments (e.g., augmentation, dropping frequent words, selective filtering) ultimately degraded model performance, as they disrupted semantic context.\n",
    "Therefore, these features were not included in the final model pipeline.\n",
    "\n",
    "#### 1.2.4Tokenizer-based Features for Transformer Models\n",
    "Different transformer models use different tokenization strategies, so the following tokenizers were explored:\n",
    "1. `RoBERTa tokenizer`\n",
    "2. `DeBERTa tokenizer`\n",
    "3. `BERT tokenizer`\n",
    "4. `GPT/RoBERTa-family tokenizers`\n",
    "\n",
    "This allowed testing how tokenization impacts emotion granularity.\n",
    "Empirical results showed that RoBERTa achieved the strongest overall performance among the transformer models used.\n",
    "\n",
    "\n",
    "---\n",
    "   <img src=\"./pics/final_f.png\" width=\"50%\">\n",
    "\n",
    "    (final data frame)\n",
    "---\n",
    "\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "I implemented a transformer-based classification model built upon BERT-base-uncased with a custom Multi-Layer Perceptron (MLP) classification head. The overall modeling workflow is summarized below:\n",
    "\n",
    "1. Train‚Äìvalidation split\n",
    "The original training set was divided using a 75/25 ratio to obtain a validation subset while preserving label distribution. This allowed monitoring of model generalization during training.\n",
    "\n",
    "2. Tokenizer and Input Encoding\n",
    "Text data was processed using the BERT tokenizer with:   \n",
    "`max_length = 128`,  \n",
    "`padding to fixed length`,  \n",
    "`truncation for long sequences` Each input sample was converted into input_ids, attention_mask, and (when applicable) token_type_ids.\n",
    "\n",
    "3. Custom BERT-based Architecture\n",
    "A customized neural network was built by:  \n",
    "    * loading bert-base-uncased from HuggingFace  \n",
    "    * extracting the [CLS] token representation as the sentence embedding  \n",
    "    * adding a two-layer MLP head (768 ‚Üí 256 ‚Üí 6) with ReLU activation and dropout  \n",
    "    * applying a final softmax through a linear layer for the six emotion classes  \n",
    "\n",
    "4. Training Strategy\n",
    "    * Optimizer: AdamW with learning rate 5e-5\n",
    "    * Loss: CrossEntropyLoss\n",
    "    * Batch size: 16\n",
    "    * Epochs: 3\n",
    "    * GPU acceleration when available\n",
    "The model was trained using mini-batch gradient descent with tqdm progress bars to track loss and accuracy in real time.\n",
    "\n",
    "5. Validation and Model Selection\n",
    "After each epoch, validation loss and accuracy were computed.\n",
    "The best model (highest validation accuracy) was saved as best_model.pth.\n",
    "\n",
    "6. Final Prediction on Test Set\n",
    "The trained BERT model produced probability logits for the test set, followed by argmax to obtain class predictions.\n",
    "Predictions were mapped back to emotion labels:\n",
    "`anger, disgust, fear, sadness, surprise, joy`\n",
    "and exported as submission.csv.\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "I have attempted various methods to analyze and improve the dataset. The approaches I experimented with include:\n",
    "\n",
    "1. Using LLaMA to generate sentence embeddings, followed by UMAP visualization.The results showed that most labels are difficult to distinguish from one another, especially the disgust and anger pair, which tend to cluster together.\n",
    "\n",
    "2. Examining high-frequency and low-frequency words to determine whether they could be used for data augmentation or removed during training.\n",
    "However, applying these strategies ultimately worsened the model‚Äôs performance.\n",
    "\n",
    "3. Constructing a TF‚ÄìIDF matrix to capture prominent words in the dataset.\n",
    "This representation was also used as input for model training.\n",
    "\n",
    "4. Combining embeddings with TF‚ÄìIDF features and training traditional machine-learning models such as KNN, Linear SVC, and LightGBM.\n",
    "\n",
    "5. Using transformer-based models such as BERT and experimenting with different MLP architectures on top of BERT outputs.This included adjusting embedding sizes, varying input text formats, adding or removing class weights, and trying related models such as DeBERTa, GPT-based models, and RoBERTa.\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "1. Based on experiments using both weighted and unweighted training, I observed that models without sample weighting consistently performed better. I suspect this is because the hidden test distribution may preserve similar class frequencies (and therefore similar misclassification tendencies), making natural class imbalance preferable to forced rebalancing. In other words, enforcing balanced learning may harm performance if the test set itself is not balanced.\n",
    "\n",
    "2. From the confusion matrix analysis, the model performs best on the joy category, but also exhibits a clear prediction bias toward joy and anger. As a result, many ambiguous or weakly emotional sentences are misclassified into these two categories. Meanwhile, the model shows limited capability in recognizing minority classes such as disgust, fear, and sadness. The surprise category is also frequently confused due to its intermediate emotional valence between positive and negative affect. Overall, the model is stable on high-frequency classes but struggles with fine-grained emotional distinctions, suggesting the need for stronger semantic representations, emotion lexicons, or more balanced training strategies.\n",
    "\n",
    "\n",
    "    <img src=\"./pics/confusion_m.png\" width=\"35%\">\n",
    "\n",
    "\n",
    "3. Regarding the transformer-based models tested, most of them achieved similar performance (around 0.65‚Äì0.68), except for variants where common words were removed‚Äîthis severely damaged semantic structure and further degraded performance. Among the tested models, RoBERTa achieved the best overall results. When sample weighting was used, these models performed slightly better on low-frequency classes, so in the final model fusion stage, I assigned higher weights to their predictions on minority categories, which resulted in a small but consistent improvement in accuracy.\n",
    "\n",
    "4. I also examined several examples from the disgust class, which had the weakest performance. Many of these examples are inherently difficult‚Äîeven humans may struggle to identify their emotions, as shown below...\n",
    "Many of these sentences contain sarcasm, fragmented expressions, ambiguous sentiment markers, or mixed emotional cues, making them inherently challenging for both machine learning\n",
    "```text\n",
    "    And I though Glory Kills were gruesome\n",
    "    Weird, just got back from the one on Grand 45 min ago and I got in and out no problem.\n",
    "    TYT is a gossip shitpost panel for race baiting idiots. By race baiting idiots.\n",
    "    What a load of crap !!\n",
    "    my school photo is honestly horrid, i look so ewüòÖ\n",
    "    Yessir, it gives me the creeps. That boy ain't in Scotland, and that boy ain't right.\n",
    "    Filthy. Stay there, please.\n",
    "    Hate to say it but having a veteran like him and [NAME] in games like this can make a difference\n",
    "    I don't know what make #Pakistan fear more their #terrorist or their #TerrorStatePak\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_file = 'final_posts.json'\n",
    "\n",
    "# Reading data from the structured json file\n",
    "with open( import_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# extract the poistion in data row\n",
    "record = []\n",
    "for item in data:\n",
    "    doc = item['root']['_source']['post']\n",
    "    record.append({\n",
    "        'post_id':doc['post_id'],\n",
    "        'text':doc['text'],\n",
    "        'hashtags':doc['hashtags']\n",
    "    }\n",
    "    )\n",
    "\n",
    "# building data frame\n",
    "df = pd.DataFrame(record)\n",
    "df = df.rename(columns={'post_id':'id'})\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target file for using terget and train_test seperation\n",
    "tag_file = 'emotion.csv'\n",
    "spl_file = 'data_identification.csv'\n",
    "\n",
    "df_tag = pd.read_csv(tag_file)\n",
    "df_spl = pd.read_csv(spl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine to form the df\n",
    "df1 = pd.merge(df, df_spl, on='id',how='outer')\n",
    "df_combine = pd.merge(df1, df_tag, on='id',how='outer')\n",
    "\n",
    "print(df_combine.head())\n",
    "print(df_combine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform the specific emotion to specific label for machine \n",
    "\n",
    "print(df_combine['emotion'].unique())\n",
    "\n",
    "label_id = { \"anger\": 0, \"disgust\": 1, \"fear\": 2,\n",
    "               \"sadness\": 3, \"surprise\": 4, \"joy\": 5 }\n",
    "\n",
    "df_combine[\"label_id\"] = df_combine.apply(\n",
    "    lambda x: label_id.get(x[\"emotion\"], None), axis=1\n",
    ")\n",
    "\n",
    "df_combine.drop(columns={\"emotion\",\"hashtags\"}, inplace=True)\n",
    "# founding that the hashtag content is already in the text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine[df_combine['split']=='train'].isnull().sum()\n",
    "# The training set is claen...\n",
    "outfile = 'df_concate.csv'\n",
    "df_combine.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "# I have tried to gen embedding by llama (spend a lot of time...)\n",
    "import ollama\n",
    "\n",
    "df = pd.read_csv(\"df_concate.csv\", encoding=\"utf-8\")\n",
    "\n",
    "def generate_embeddings( row, text_column_name='text'):\n",
    "    embeddings = ollama.embeddings(\n",
    "        model='embeddinggemma:latest',\n",
    "        prompt=row[text_column_name],\n",
    "    )\n",
    "    return embeddings[\"embedding\"]\n",
    "\n",
    "# We use the text column\n",
    "column_name = 'text'\n",
    "\n",
    "df['embeddings'] = df.apply(lambda row: generate_embeddings(row, column_name), axis=1)\n",
    "df.to_csv(\"df_concate_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 frequency word exploring..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('df_concate.csv', encoding='utf-8')\n",
    "\n",
    "def sen_to_ls( s:str )->list:\n",
    "    ls = []\n",
    "    for i in s.split():\n",
    "        ls.append(i)\n",
    "    return ls\n",
    "\n",
    "for i in range(1,6):\n",
    "    print(df[df['label_id'] == i]['text'] \\\n",
    "    .apply(sen_to_ls) \\\n",
    "    .explode() \\\n",
    "    .value_counts())\n",
    "\n",
    "# In such method, I have tried to calculating high frq, low frew and make a reduction based on alalysis\n",
    "# but the model fail to understanding whole sentnece ane then perform worse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 UMAP exploring : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I haved tried the 2-d, 3-d ...\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(\"df_concate_embedding.csv\", encoding= \"utf-8\")\n",
    "f[\"embeddings\"] = df[\"embeddings\"].apply(lambda x: np.array(ast.literal_eval(x), dtype=np.float32))\n",
    "x_embedding = np.array(df[\"embeddings\"].tolist())\n",
    "reducer = umap.UMAP(n_components=3, metric='cosine', random_state=28) \n",
    "embedding_3d = reducer.fit_transform(x_embedding)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_plot = pd.DataFrame(embedding_3d, columns=['UMAP1', 'UMAP2', 'UMAP3'])\n",
    "df_plot['emotion'] = df['label_id']\n",
    "df_plot['text'] = df['text']\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    df_plot,\n",
    "    x='UMAP1',\n",
    "    y='UMAP2',\n",
    "    z='UMAP3',\n",
    "    color='emotion',  # Color points by the 'emotion' column\n",
    "    hover_data=['text'],  # Show text and intensity on hover\n",
    "    title='3D UMAP Projection of Text Embeddings'\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")\n",
    "\n",
    "## It is quit mixed!!! Nearly can I analysis them..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the pattern in th train df to create the tf-udf matrix\n",
    "\n",
    "df_train = df[df[\"split\"] == \"train\"].copy()\n",
    "df_test  = df[df[\"split\"] == \"test\"].copy()\n",
    "\n",
    "# ---------------------------------------\n",
    "#       TF-IDF Fit on TRAIN Only\n",
    "# ---------------------------------------\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1,2),\n",
    "    max_features=10000,\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit on train\n",
    "tfidf.fit(df_train[\"text\"])\n",
    "\n",
    "# Transform train/test\n",
    "train_tfidf = tfidf.transform(df_train[\"text\"])\n",
    "test_tfidf  = tfidf.transform(df_test[\"text\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_tfidf_df = pd.DataFrame(\n",
    "    train_tfidf.toarray(),\n",
    "    columns=tfidf.get_feature_names_out(),\n",
    "    index=df_train.index\n",
    ")\n",
    "\n",
    "test_tfidf_df = pd.DataFrame(\n",
    "    test_tfidf.toarray(),\n",
    "    columns=tfidf.get_feature_names_out(),\n",
    "    index=df_test.index\n",
    ")\n",
    "\n",
    "# df_train / df_test\n",
    "df_train = pd.concat([df_train, train_tfidf_df], axis=1)\n",
    "df_test  = pd.concat([df_test, test_tfidf_df], axis=1)\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "\n",
    "# finally I try to combine the embedding + tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Final model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 import module and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the module\n",
    "import torch as tch\n",
    "import tqdm\n",
    "import transformers\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = '/content/df_concate.csv'\n",
    "df = pd.read_csv(ifile)\n",
    "df_train = df[ df[\"split\"]=='train'].copy()\n",
    "df_test = df[ df[\"split\"]=='test'].copy()\n",
    "print(df_train)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "val_ratio = 0.25\n",
    "\n",
    "train_text, val_text,  train_label, val_label = train_test_split(\n",
    "    df_train['text'], df_train['label_id'], random_state= seed, test_size=val_ratio\n",
    ")\n",
    "\n",
    "train_label = train_label.values\n",
    "val_label = val_label.values\n",
    "print(train_text.shape)\n",
    "print(train_label.shape)\n",
    "print(val_text.shape)\n",
    "print(val_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(train_text.tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "val_encodings = tokenizer(val_text.tolist(), truncation=True, padding='max_length', max_length=128)\n",
    "test_encodings = tokenizer(df_test[\"text\"].fillna(\"\").tolist(), truncation=True, padding='max_length', max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmoDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: tch.tensor(val[idx], dtype=tch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = tch.tensor(self.labels[idx], dtype=tch.long)\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EmoDataset(train_encodings, train_label)\n",
    "val_dataset = EmoDataset(val_encodings, val_label)\n",
    "train_loader = tch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = tch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 weighted balancing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted balancing\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "# calculating the class weight..\n",
    "classes = np.unique(df_train['label_id'].dropna())\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=classes,\n",
    "    y=df_train['label_id'].dropna()\n",
    ")\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "\n",
    "device = tch.device('cuda' if tch.cuda.is_available() else 'cpu')\n",
    "# transform to tensor\n",
    "weights = tch.tensor(class_weights, dtype=tch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class CustomBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_labels=6, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout(cls_output)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 training setting and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "num_epochs = 3 \n",
    "best_val_acc = 0.0 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=True)\n",
    "    for step, batch in enumerate(progress_bar, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # batch acc ,avg loss\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "        batch_correct = (preds == batch[\"labels\"]).sum().item()\n",
    "        batch_total = batch[\"labels\"].size(0)\n",
    "        batch_acc = batch_correct / batch_total\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "        avg_loss = total_loss / step\n",
    "\n",
    "        # tqdm\n",
    "        progress_bar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"avg_loss\": f\"{avg_loss:.4f}\",\n",
    "            \"batch_acc\": f\"{batch_acc:.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_train_acc = correct / total\n",
    "    print(f\"Train | Loss: {avg_train_loss:.4f} | Acc: {avg_train_acc:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_bar, start=1):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "           \n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "            batch_correct = (preds == batch[\"labels\"]).sum().item()\n",
    "            batch_total = batch[\"labels\"].size(0)\n",
    "            batch_acc = batch_correct / batch_total\n",
    "            val_correct += batch_correct\n",
    "            val_total += batch_total\n",
    "            avg_val_loss = val_loss / step\n",
    "            avg_val_acc = val_correct / val_total\n",
    "\n",
    "            val_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"avg_loss\": f\"{avg_val_loss:.4f}\",\n",
    "                \"batch_acc\": f\"{batch_acc:.4f}\",\n",
    "                \"avg_acc\": f\"{avg_val_acc:.4f}\"\n",
    "            })\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "    print(f\"Validation | Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"Saved new best model! (Acc: {best_val_acc:.4f})\")\n",
    "    else:\n",
    "        print(f\"Best model remains at {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for batch in val_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "    y_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "    y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "print(f\"Macro-F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EmoDataset(test_encodings, [0] * len(df_test))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
    "        outputs = model(**batch)\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "        predictions.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "id2label = {0: \"anger\", 1: \"disgust\", 2: \"fear\", 3: \"sadness\", 4: \"surprise\", 5: \"joy\"}\n",
    "df_test[\"emotion\"] = [id2label[p] for p in predictions]\n",
    "\n",
    "df_test[[\"id\", \"emotion\"]].to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSubmission file saved as submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Other trial\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "import plotly.express as px\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv(\"df_concate_embedding.csv\", encoding= \"utf-8\")\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "df[\"embeddings\"] = df[\"embeddings\"].apply(lambda x: np.array(ast.literal_eval(x), dtype=np.float32))\n",
    "\n",
    "x_embedding = np.array(df[\"embeddings\"].tolist())\n",
    "reducer = umap.UMAP(n_components=2, metric='cosine', random_state=28) \n",
    "embedding_2d = reducer.fit_transform(x_embedding)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_plot = pd.DataFrame(embedding_2d, columns=['UMAP1', 'UMAP2'])\n",
    "df_plot['emotion'] = df['label_id']\n",
    "df_plot['text'] = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast #astype\n",
    "\n",
    "df = pd.read_csv(\"df_concate_embedding.csv\")\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace= True)\n",
    "df[\"embeddings\"] = df[\"embeddings\"].apply(lambda x: np.array(ast.literal_eval(x), dtype=np.float32))\n",
    "df_train = df[df['split'] == \"train\"]\n",
    "df_test = df[df['split'] == \"test\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = tts( df_train[\"embeddings\"], df_train[\"label_id\"], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "knn.fit(X_train.tolist(), y_train)\n",
    "\n",
    "# Predicting the label for the test data\n",
    "y_pred = knn.predict(X_val.tolist())\n",
    "\n",
    "#Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(f'KNN Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_val, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train.tolist(), y_train)\n",
    "y_pred = clf.predict(X_val.tolist())\n",
    "print(classification_report(y_true=y_val, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(f'KNN Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(X_train.tolist(), y_train)\n",
    "y_pred = clf.predict(X_val.tolist())\n",
    "print(classification_report(y_true=y_val, y_pred=y_pred))\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "print(f'KNN Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2025-Lab2-Exercise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
